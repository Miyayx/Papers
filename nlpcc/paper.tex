\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}
\usepackage{CJK}
\usepackage{url}
\usepackage[ruled]{algorithm2e}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumerate}
\usepackage{multirow}

\usepackage{xeCJK}
%\usepackage[BoldFont,SlantFont,CJKchecksingle]{xeCJK}
\setCJKmainfont[BoldFont=STHeiti, ItalicFont=STKaiti]{STSong}
\setCJKsansfont[BoldFont=STHeiti]
\setCJKmonofont{STFangsong}
%\usepackage{CJKutf8}

%\usepackage[BoldFont,SlantFont,CJKchecksingle]{xeCJK}
%\setCJKmainfont[BoldFont=SimHei]{SimSun}
%\setCJKmonofont{SimSun}% 设置缺省中文字体

\urldef{\mailthu}\path|{lmy13}@tsinghua.edu.cn|
\urldef{\mailsy}\path|{fantasysy}@sina.com|
\urldef{\mailgmail}\path|{meeya.yx, shiyao, wangzigo}@gmail.com|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip\noindent\keywordname\enspace\ignorespaces#1}
\newcommand{\para}[1]{\vspace{0.1cm}\noindent\textbf{#1}}

\begin{document}
%\begin{CJK*}{UTF8}{song}
\mainmatter

\title{Building Large-Scale Cross-Lingual Knowledge Base from Heterogeneous Online Wikis}
%\titlerunning{Bilingual Knowledge Base}
%\author{Mingyang Li$^\dag$ \and Yao Shi$^\dag$ \and Zhigang Wang$^\dag$}
%\authorrunning{Mingyang Li et.al}

%\institute{$^\dag$ Department of Computer Science and Technology,\\
%Tsinghua University, Beijing 100084, China\\
%\mailgmail\\
%}

\maketitle

\begin{abstract}
Cross-lingual Knowledge Bases are important for global knowledge sharing. However, there are few Chinese-English knowledge bases due to the following reasons: 1) the scarcity of Chinese knowledge; 2) the number of current cross-lingual language links is limited; 3) the incorrect relation in semantic taxonomy. In this paper, a large-scale cross-lingual knowledge base(CLKB) is build to solve the above problems. The CLKB is based on English and Chinese Wikipedia as well as Baidu Baike and Hudong Baike. An extension method is used to increase language links while a pruning approach is used to refine taxonomy. Totally, there are 663,740 classes, 11,721,238 instances, 56,449 properties referred in the CLKB. Among of them, 507,042 items are cross-lingual linked. To the best of our knowledge, it's the first large-scale Chinese-English knowledge base with balanced knowledge quantity. Moreover, the paper provides visualization of knowledge and a SPARQL endpoint accessing to the established CLKB.

\keywords{Knowledge Base, Semantic Web, Taxonomy, Cross-lingual}
\end{abstract}

\section{Introduction}
As the Web is evolving to a highly globalized information space, sharing knowledge across different languages is attracting increasing attentions. Multilingual knowledge bases have significant applications such as multi-lingual information retrieval, machine translation and deep question answering. DBpedia, by extracting structured information from Wikipedia\footnote{{\tt http://www.wikipedia.org/}}, is a multi-lingual multi-domain knowledge base and becomes the nucleus of LOD\footnote{{\tt http://linkeddata.org/}}. Obtained from the automatic integration of WordNet\footnote{{\tt http://wordnet.princeton.edu/}} and Wikipedia, YAGO, MENTA and BabelNet are other famous large multi-lingual knowledge bases.

However, most non-English knowledge in those knowledge bases is pretty scarce. Figure \ref{fig_stat} shows a simplified long tail distribution of the number of articles on six major Wikipedia language versions. Due to the imbalance of different Wikipedia language versions, the knowledge distribution across different languages is highly unbalanced in those knowledge bases which are generated from Wikipedia. For instance, DBpedia contains 4.58 million English instances but even no Chinese dataset published. On the other hand, the Chinese Hudong Baike\footnote{{\tt http://www.baike.com/}} and Baidu Baike\footnote{{\tt http://baike.baidu.com/}}, both containing more than 6 million articles, are even larger than the English Wikipedia. If a knowledge base could be established between English Wikipedia and Chinese Hudong Baike, an Chinese-English knowledge base with much higher coverage in Chinese can be constructed.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75 \columnwidth]{fig/fig_stat.png}
\caption{Number of Articles on Major Wikipedias, Baidu Baike and Hudong Baike}
\label{fig_stat}
\end{figure}

To enrich the Chinese knowledge, we try to build a large-scale Chinese-English knowledge base by semantifying four heterogeneous online wikis, which are English Wikipedia, Chinese Wikipedia, Hudong Baike and Baidu Baike. In this paper, we propose a unified framework to build such a knowledge base. The framework contains three steps: extracting dataset from online wikis, extending an initial language link set, and pruning taxonomy for more precise semantic relation. The generated knowledge base contains 663,740 classes, 11,721,238 instances and 56,449 properties. An online system supporting keyword search and SPARQL endpoint is also developed.

This non-trivial task poses the challenges as follows:
\begin{enumerate}
  \item The cross-lingual links are highly limited inside Wikipedia. For instance, there are only 9\% Chinese-English matched articles in Wikipedia in all articles. How could we find enough Chinese-English \verb"owl:sameAs" relations?
  \item The subsumption relations of the online wikis' category systems contain lots of noise. For example, in English Wikipedia "Wikipedia-books-on-people", which is actually \verb"subClassOf" "Books", is taken as the sub-category of "People" mistakenly. How could we detect those incorrect semantic relations?
\end{enumerate}

Driven by these challenges, we propose a unified framework to build a Chinese-English knowledge base from four heterogeneous online wikis. Specifically, our work makes the following contributions.
\begin{enumerate}
  \item We achieve refined datasets from multiple heterogeneous online wikis through unified extracting process.
  \item We extend cross-lingual link set by employing a cross-lingual knowledge linking discovery approach for class and instance, and analyzing templates in Wikipedia for property.
  \item We prune the original taxonomy, which is extracted from wiki category system, to retrieve more precise \textit{subClassOf} and \textit{instanceOf} relations.
  \item Both online-system and SPARQL endpoint are provided for public query operations over our knowledge base.
\end{enumerate}

The rest of the paper is organized as follows. Section \ref{sec:pd} presents some basic concepts and the problem formulation. In Section \ref{sec:sde} we present detailed approaches. Section \ref{sec:cli} describes cross-lingual integration. The experimental results are reported in Section \ref{sec:result}. Some related work are given in Section \ref{sec:work}. Finally we conclude our work in Section \ref{sec:con}.

\section{Preliminaries}
\label{sec:pd}
In this section, we give definitions about our knowledge base and describe the task in this work.

\subsection{Basic Concepts}
\para{Online Wikis.} Nowadays, Wikipedia is the largest data store of human knowledge. It was launched in 2001 and has hold over 35 million articles in 288 languages by 2015. Out of these, English articles contribute most. The imbalance of different language articles makes ontologies based on Wikipedia-only behave badly in cross-lingual. Thus, more Chinese sources are necessary to enrich Chinese source.

Among the large-scale monolingual Chinese wikis currently, Baidu Baike and Hudong Baike are the most content-rich. Hudong Baike was founded in 2005 and contains more than 12 million articles with about 9 million experts' contribution until 2015. Meanwhile, Baidu Baike maintains more than 11 million articles.

Here, we consider an encyclopedia wiki as a collection of articles, category system, which can be defined as: $W = <A,C>$, where A denotes articles, C denotes categories in W.

\para{Wiki Pages.} Articles from the wiki sources are similar in structure. Usually they provide two important elements with potential semantic information, category system and articles. A category system represents the relations between categories as a tree by the relation \textit{subCategoryOf}. An article describes an entity with rich information. Each article is linked to one or more categories by \textit{articleOf} relation. In general, six elements can be exploited in each article page:
%\begin{figure}
%    \centering
%    \begin{minipage}[t]{0.8\textwidth}
%        \centerline{\includegraphics[width=0.8\columnwidth]{fig/hudong-taxonomy2}}
%        \caption{Taxonomy in Hudong}
%        \label{fig:hudong-taxonomy}
%    \end{minipage}%
%\end{figure}
\begin{itemize}
  \item Title: A Title is the label of an entity, whose uniqueness can be used to distinguish entities.
  \item Abstract: An abstract is a brief summary of the entity. It's always the first paragraph of an article.
  \item Infobox: Most of articles contain infobox. An infobox maintains structured data in subject-attribute-value format.
  \item Link: Links are entries to other articles within the wiki. They represent the relations between the current article and others.
  \item Category: The categories that an article belongs to are usually listed at the bottom of article page, shown as tags. An article has \textit{articleOf} relation with its categories.
  \item URL: Each article has an HTTP url to identify itself on web.
\end{itemize}

%Fig. \ref{fig:interstellar} shows a snap of an article in Chinese Wikipedia.
%\begin{figure}[ht]
%    \centerline{\includegraphics[width=1\columnwidth]{fig/interstellar}}
%    \caption{A snap of Interstellar(Film) article in Chinese Wikipedia}
%    \label{fig:interstellar}
%\end{figure}%
%
An article $a$ can be defined as follow:
\begin{equation}
    a = <Ti(a),Ab(a),Li(a),In(a),C(a),U(a)>
\end{equation}
where $Ti(a),Ab(a),Li(a),In(a),C(a),U(a)$ denotes title, abstract, links, infobox, category tags, url of article $a$.

Notably, articles in Wikipedia follow templates specified by Wikipedia when being edited. A template defines items that a group of article should fill. Besides, infoboxes are also generated based on certain templates recommended by Wikipedia. For example, The infobox in film 冰雪奇缘(Frozen) is edited according to the Template \emph{Infobox film}, which maintains a property set of films. An infobox $In(a)$ contains a set of attribute-value pairs {$p_{1}$, $p_{2}$,...}. We denote infobox template used in article $a$ as $T(a)$. Templates specify certain attributes, which are usually different from those displayed on the webpage. Thus, we define an attribute-value pair as a triple $p=<tl,dl,v>$, where $tl$ is attribute label in template, $dl$ is displayed label in web page and $v$ is the corresponding value. The value maybe a text or a reference to another entity.

Moreover, in Wikipedia, some article pages have language links which help readers switch to other language-version articles. %Fig. \ref{fig:interstellar} shows language links of \emph{Interstellar} on the left column of the Wikipedia page.
As to an article $a$ containing multi-language content in Wikipedia, $L_{e}$ and $L_{z}$ denote the article's language links in English and Chinese. To a $cl \in CL$, $cl(a) = <L_{z}(a), L_{e}(a)>$.

\para{Knowledge Base} A knowledge base is a formal specification of a group of entities. Our knowledge base is described as a 4-tuple:
\begin{equation}
    KB = <C,I,P,H^C>
\end{equation}
where $C$, $I$, $P$ are the sets of classes, instances, and properties, respectively. $H^C$ represents the hierarchical relationships of classes. The taxonomy of our knowledge base includes four types of relationships, which are \textit{subClassOf} of class-class, \textit{instanceOf} of class-instance, \textit{relatedClassOf} of instance and related class, \textit{relatedTopicOf} of class and related topic.

A Cross-lingual Knowledge Base(CLKB) is a database conform to a cross-lingual ontology. Taking advantage of language links in $CL$, several monolingual knowledge bases generated from various sources can be merge into one.  Thus our Chinese-English knowledge base can be defined as:
\begin{equation}
    CLKB = <KB_{z}, KB_{e}>
\end{equation}
where $KB_{z}$ and $KB_{e}$ denote the monolingual knowledge base in Chinese and English. CLKB combines the two according to $CL$.

\para{Cross-lingual Knowledge Base Building} In this paper, our task is to build a CLKB assembling knowledge from several English or Chinese wiki sources. Given an online-wiki $W_{i}$, we get dataset including class list $C_{i}$, instance list $I_{i}$, property list $P_{i}$, taxonomy $H^C_{i}$ of each $W_{i}$ by extraction. We then enrich the extracted cross-lingual link set $CL$ using an link-discovery extension method. We also refining taxonomy by checking if an \textit{articleOf} or \textit{subCategoryOf} is really an \textit{instanceOf} or \textit{subClassOf} relationship. Our final output is an Chinese-English CLKB though process of combining $KB_{z}$ and $KB_{e}$ by utilizing language links in $CL$.

The whole building procedure is shown in Fig. \ref{fig:procedure}. We extract information from four sources, Baidu Baike, Hudong Baike, Chinese Wikipedia and English Wikipedia. Because of heterogeneity, various extractors should be employed. After data parsing, we delete incorrect semantic relations by going through taxonomy pruning. At last, using the extended cross-lingual links, we combine the four dataset into our CLKB.

\vspace{-0.5cm}
\begin{figure}[ht]
    \centerline{\includegraphics[width=1\columnwidth]{fig/procedure2}}
    \caption{Procedure of Building Our Cross-lingual Knowledge Base}
    \label{fig:procedure}
\end{figure}
\vspace{-0.5cm}

\section{Semantic Data Extraction}
\label{sec:sde}
Semantic data extraction aims to achieve a structured dataset from the input wikis. Specifically, we extract classes from category system, instances according to articles, and properties based on infoboxes and their templates.

\subsection{Class Extraction}
\label{sec:ce}
A class is defined as a type of similar instances. For example, the class of instance \emph{Frozen} is \emph{Film}. In general, a class has super classes and sub classes. Classes comprise a taxonomy which presents a backbone of an ontology. In a wiki, a category groups several articles and also has super-categories and sub-categories. Therefore we can extract classes based on existing category system.

However, the whole taxonomy can not directly transform from category system due to the following problems:
\begin{itemize}
    \item There are auxiliary categories in Wikipedia, which help arrange specific articles or category pages. For example, \emph{Lists of artists} or \emph{Food templates}.
    %s\item Some sub-category links in the category system maybe inconsistent. Some categories may contain itself as sub-category, or contain sub-category that also be the super-category of it. As Fig. \ref{fig:category-mistakes} shows: In Hudong, the sub-category of 国家元首(Head of State) contains itself as a child, which causes a circle in taxonomy tree. Meanwhile, in Wikipedia, \textcolor{red}{例子}
    \item Some categories relate to only one article. According to the definition of class, such categories are less representative to a group of instances, therefore it's unwise to retain them as classes.
\end{itemize}

To obtain a more precise $H^C$ in a wiki, we remove categories fit the above situations, then build the original class hierarchy $H^C$ using the remaining categories.

%However, among the relations there are still incorrect samples. \textcolor{red}{example}
%For example, \emph{Frozen} is not an entity of \emph{Disney Princess}, but relates to. Thus we will prune the taxonomy later.

\subsection{Property Extraction}
\label{sec:pe}
A property is defined as an attribute of an entity. It represents the relation between an instance and its value. We divided properties into two types: object property, whose value is an individual, such as 导演(directed by); datatype property, whose value is a literal text, such as 生日(birth date). Considering both content and infobox of an article, we extract two kinds of properties, General-properties and Infobox-properties.

\para{General-properties.} Characteristics of an entity are regarded as general properties, including label, abstract, and url. General-properties describe general information of an entity. We define three datatype properties as general-properties for a given article $a$: (1) label property; (2) abstract property; (3) URL property.

\para{Infobox-properties.} Attributes acquired from infobox are considered as infobox properties, such as 上映时间(release date), 导演(directed by) in a movie's infobox. The type of a property, datatype or object, depends on the type of the value. Ordinarily, a plain text value marks the property as datatype while an entity reference determines the property as object. For example, the attribute 上映时间(release date) can be defined as a datatype property as its value is a datetime string. Meanwhile, 导演(directed by) is an object property because its value points to a person who directed the movie.

We are challenged when extracting properties from infoboxes:
\begin{itemize}
    \item In Wikipedia, the attribute label displayed in the webpage infobox is inconsistent with it in the published dump file. Fig.\ref{fig:infobox-template} gives a mapping result of display labels and dump labels in \emph{Frozen's} infobox. The left is infobox, the middle is a glance from dump file in Wikipedia. We explore the display labels as property labels in Wikipedia rather than dump labels extracted from raw data.
    \begin{figure}[ht]
        \centerline{\includegraphics[width=1\columnwidth]{fig/infobox-template}}
        \caption{Comparison of display label and dump label in \emph{Interstellar} infobox}
        \label{fig:infobox-template}
    \end{figure}%
\item There are special characters in labels. Wikipedia usually uses hyphen "-" or dot "•" to mark sublabels. For example, attribute \emph{population} property has sub-properties "-Density" and "-Urban". In addition, odd signs, such as colon or asterisk, may occur in Baidu or Hudong property labels by mistake.
\end{itemize}

To solve the problems above, we take advantage of template information. Specifically, Wikipedia institutes rules of rendering label in templates. Right of Fig.\ref{fig:infobox-template} shows an example of \emph{Template:Infobox film}, where corresponding relations of display labels and dump labels come from. When a dump label occurs in dump file, we replace it by its mapping display label.

\subsection{Instance Extraction}
\label{sec:ie}
An article describes unique entity in the world. Therefore we can extract an article as an instance. During the extraction, illustrative or structure-related articles in Wikipedia are deleted, including category list pages and template documentations.

We harvest four types of information during this stage. (1) General-properties of instance, including title as label property value, first paragraph as abstract property value and HTTP URL as URL property value; (2) Infobox-properties which are acquired via extracting from the infobox in the article; (3) $articleOf$ relation with categories listed at the bottom of article page. For example, 美国电影作品(American films) is a category of 冰雪奇缘(Frozen); (4) Reference relation with other instances according to links in the content, such as 冰雪女王(The Snow Queen).

\section{Cross-lingual Integration}
\label{sec:cli}

To construct a CLKB with existing structured data, firstly we increase cross-lingual links, which can help match the same entity in two languages. Secondly, we integrate this four wikis, that is, respectively merging classes, instances and properties from the four sources if they represent the same thing. Thirdly, we prune the taxonomy generated from class relationships to make it more accurate.

\subsection{Cross-lingual Linking}
There are 227 thousand cross-lingual links between English and Chinese, which constitute the initial cross-lingual link set of classes and instances. Moreover, we utilize the language-independent method in \cite{wang2012cross} to extend the language-link set. With the linkage factor graph model, we harvest a cross-lingual links extension as many as 215 thousand with an ideal precision 85.5\% and a recall of 88.1\% between English Wikipedia and Baidu Baike.

However, due to using templates, Infobox-properties have no obvious cross-lingual links. Thus, we take the following steps to acquire property links:
\begin{enumerate}
    \item Given two matched templates, $T_{e}$ and $T_{z}$, find the display labels mapping the same dump label. That is to say, to $p_{e}$ in $T_{e}$ and $p_{z}$ in $T_{z}$, if $tl_{e}$ is equal to $tl_{z}$, $<dl_{e},dl_{z}>$ are cross-lingual properties;
    \item Given the English and Chinese infoboxes of two matched articles $a_{e}$ and $a_{z}$ direct to the same entity, compare their templates, English template $T_{e}(a_{e})$ and Chinese template $T_{z}(a_{z})$, find the matched display labels mapping to the same dump label;
    \item Given the English and Chinese infoboxes of a matched instance, for datatype properties, compare the similarity of literal value; for object properties, check whether the value refer to the same entity.
\end{enumerate}

In order to make all wikis link to each other, we unify the same class, instance and property from four sources, and give them unique identifiers. For instance, we merge instances by the following steps: (1) Merge all instances extracted from Chinese wikis by instance title. (2) To a $L_{z}$, find whether there is an English cross-lingual link $L_{e}$ in $CL<L_{e}, L_{z}>$. If exists, make the two as one instance and identify it using an ID, else number it with a new ID.

The process of unifying class and property is the same as instance. Meanwhile, all the relations are kept to prevent loss of information.

\subsection{Taxonomy Prune}

As a result of combining multi-source information without verifying, there is noise in taxonomy. %For example:
Therefore, we introduce the method from \cite{wang2014cross} to detect the correct \textit{subClassOf} and \textit{instanceOf} relations from \textit{subCategoryOf} and \textit{articleOf}. %Table. \ref{tab:} shows some examples of correct relations.
In particular, some language-dependent literal and language-independent structural features are defined to vectorize each class or instance. Employing these features, a Yes-or-No binary-classification model is trained based on Logistic Regression. The whole process is iterative by retraining model with assured result to get higher precision. The prediction results are validated by cross-lingual information.

The ideal result after pruning is a tree, whose edges, nodes, and leaves respectively denote relations, classes and instances. However, since getting rid of incorrect entity relations without consideration of integrity, a forest result is inevitable.

To retain integrity of semantic relation, we define two types of new relations: \textit{relatedClassOf} for cut class-instance relations and \textit{relatedTopicOf} for cut class-class relations. 

\section{Result}
\label{sec:result}

\subsection{Extracted Knowledge Base}
We collect the resources from four wiki sites, English and Chinese Wikipedia dump files in May, 2014, Hudong html pages until May, 2014, and Baidu html pages until September, 2014. Each of the wikis has three types of information, which can be utilized for constructing our knowledge base, namely, specific articles, category system, and attribute of articles. We extract each raw data, then form the extracted information into well-structured data. Table \ref{tab:extract-result} the result we get after elementary extraction on 4 different wiki sources. Besides, we obtain 227 thousand language links between English and Chinese in Wikipeidia, and increase the number by 215 thousand enwiki-baidu language links and 10 thousand property links.
\vspace{-0.5cm}
\begin{table}[hb]
    \small
    \centering
    \caption{Statistics of elementary extraction result}
    \label{tab:extract-result}
        \begin{tabular}{|l|l|l|l|l|}
            \hline
                       & Enwiki    & Zhwiki   & Hudong    & Baidu     \\ \hline
            \#Instance & 4,304,113 & 662,650  & 5,590,751 & 5,622,404 \\ \hline
            \#Class    & 982,432   & 159,705  & 31,802    & 1300      \\ \hline
            \#Property & 43,976    & 18,842   & 1187      & 139,634   \\ \hline
        \end{tabular}
\end{table}

We create URIs \url{http://clkb/type/id} (type could be \textit{class}, \textit{instance}, \textit{property}) to identify each element and provide corresponding information if users look up elements over HTTP protocol to achieve the knowledge base.

After fusing the heterogeneous resources, we harvest a cross-lingual graph with 663,740 classes, 56,449 properties, and 11,721,238 instances respectively. With different methods of extraction and language link discovery, these three kinds of entries show different results in languages. We give a breakdown of both Chinese knowledge and English knowledge in Table \ref{tab:kb-result}.
\vspace{-0.5cm}
\begin{table}[ht]
\small
\centering
\caption{Statistics of our knowledge base}
\label{tab:kb-result}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{} & \multicolumn{2}{c|}{Classes}     & \multicolumn{2}{c|}{Instance}                   & \multicolumn{2}{c|}{Property}    \\ \hline
English                & 639,020 & 96.26\%                & 3,879,121              & 38.79\%                & 15,380  & 27.24\%                \\ \hline
Chinese                & 88,615  & 13.35\%                & 7,409,519              & 68.25\%                & 51,618  & 91.44\%                \\ \hline
Cross-lingual          & 63,895  & 9.63\%                 & 432,598                & 3.98\%                 & 10,549  & 18.69\%                \\ \hline
Only English           & 575,125 & 86.65\%                & 3,446,523              & 31.75\%                 & 4,831   & 8.56\%    \\ \hline
Only Chinese           & 24,720  & 3.72\%                 & 7,409,519              & 64,27\%                  & 41,069  & 72.75\%   \\ \hline
Total                  & 663,740 & \multicolumn{1}{c|}{-} & 11,721,238             & \multicolumn{1}{c|}{-} & 56,449  & \multicolumn{1}{c|}{-} \\ \hline
\end{tabular}
\end{table}
\vspace{-0.5cm}

Our knowledge graph is organized in Openlink Virtuoso, which is a data management platform rendering various services, including triple store.

\subsection{Web Access to Knowledge Base}

\begin{figure}[ht]
    \centerline{\includegraphics[width=1\columnwidth]{fig/xlore}}
    \caption{Sample Pages of Class, Instance and Property}
    \label{fig:xlore}
\end{figure}

We provide a platform to present an intuitive graph of our knowledge in the forms of class, instance and property. Fig.\ref{fig:xlore} shows sample pages of the integrated data. Users can choose language preference which is convenient for both English speaker and Chinese speaker. In the class webpage, we exhibit the label, super classes, subclasses, related topics, properties and instances of the specific class in bilingual way on condition that the corresponding English entries or Chinese entries exist.  In the instance webpage we display bilingual label, super classes, related classes, abstracts, infobox-properties, images and references. In the property webpage, we present bilingual label, domains, ranges, and related instances of each property.

\begin{figure}
\centering
\subfigure[A picture of Search Box]{
    \label{fig:search-engine}
    \includegraphics[width=5.5cm,height=3cm]{fig/search-engine}
    }
\hspace{0.01cm}
\subfigure[A SPARQL Sample Query]{
    \label{fig:sparql-endpoint}
    \includegraphics[width=5.5cm,height=3cm]{fig/sparql-endpoint}
    }
\caption{Two ways to our knowledge base}
\label{fig:access}
%\begin{minipage}[t]{0.5\textwidth}
%    \centerline{\includegraphics[width=5.5cm,height=3cm]{fig/search-engine}}
%    \caption{A picture of Search Box}
%    \label{fig:search-engine}
%\end{minipage}%
%\begin{minipage}[t]{0.5\textwidth}
%    \centerline{\includegraphics[width=5.5cm,height=3cm]{fig/sparql-endpoint}}
%    \caption{A SPARQL Sample Query}
%    \label{fig:sparql-endpoint}
%\end{minipage}%
\end{figure}

Beside these user-friendly pages, we provide two ways to access our knowledge base shown in Fig.\ref{fig:access}: via the search engine or via SPARQL endpoint. For general users, they can make a query by inputing related text into searchbox and search to get entities with similar label. To present practicable result, An index is generated over all entities. We as well provide SPARQL interface for professional users to query our knowledge graph. Users can choose the language tags of their desired results by \textbf{"filter(langMatches(?label),"en"))"} or \textbf{"filter(langMatches (?label),"zh"))"}.%For example, to get the English label of an instance, the query is: select * where {$<$\url{http://clkb/instance/100}$>$ $<$\url{http://www.w3.org/2000/01/rdf-schema#label}$>$ ?label. filter(langMatches(?label),"en"))}.


\section{Related Work}
\label{sec:work}
In this section, we introduce some related knowledge bases and cross-lingual knowledge linking methods.

\para{Chinese Knowledge Bases.} Currently, several large-scale Chinese knowledge bases have been generated. Zhishi.me\cite{niu2011zhishi,wang2014publishing} is the first published Chinese large-scale Linking Open Data. It acquires structural information from three original sources, Chinese Wikipedia, Baidu Baike and Hudong Baike and gains more than 5 million distinct entities. Zhishi.me helps generate knowledge base focused on relations in Junfeng Pan’s work\cite{pan2012building}.

Similar with Zhishi.me, CKB\cite{wang2012building} is created from Hudong Baike. It first learns an ontology based on category system and properties, and then collects 19,542 classes, 2,381 properties, 802,593 instances. Besides using existing online-wikis, CASIA-KB employs other types of sources(e.g. microblog posts, news pages, images) to enrich the structured knowledge.

\para{Cross-lingual Knowledge Bases.} DBpedia \cite{auer2007dbpedia,mendes2012dbpedia} is one of the most used cross-lingual knowledge base in the world. It extracts various kinds of structured information from Wikipedia and employ the multilingual characteristic of Wikipedia to generate 97 language versions of content. This knowledge base is widely applied in many domains, including media recommendation \cite{fernandez2011generic,kaminskas2012knowledge}, entity linking\cite{mendes2011evaluating} and information extraction \cite{dutta2013integrating}. Universal WordNet(UWN)\cite{de2012uwn} is a large multilingual lexical knowledge base which is build from WordNet and enriched its entities from Wikipedia. It is constructed using sophisticated knowledge extraction, link prediction, information integration, and taxonomy induction methods. The API is available to over 200 languages and more than 16 million words and names. UWN provides semantic relationship of list of word meanings for Aya's work on classual search \cite{al2015conceptual}

%\subsection{Cross-lingual Knowledge Linking}
%Discovering more cross-lingual links is benefit to development of a multilingual knowledge base. General method is divided into two steps. First find missing link candidates using link structure of articles and then decide whether candidates are cross links or not by classification. \cite{sorg2008enriching} employs such approach to resolve the problem of automatically inducing new cross-language links. Wang\cite{wang2012cross} utilize a linkage factor graph model

\section{Conclusion}
\label{sec:con}
This paper presents a procedure of building a Chinese-English CLKB from four wiki sources. At first, we extract structured information and unify data format. Then a cross-lingual language link set is generated and expanded to help combine the bilingual sources. To refine our dataset, we also conduct pruning work on taxonomy. Finally, we acquire a CLKB containing 663,740 classes, 11,721,238 instances and 56,449 properties. Currently, a website and SPARQL query interface is provided to access the knowledge base.

%\section*{Acknowledgement}
%Thanks anonymous reviewers for their valuable suggestions that help us improve the quality of the paper. Thanks Prof. Chua Tat-Seng from National University of Singapore for discussion. The work is supported by 973 Program (No. 2014CB340504 ), NSFC-ANR (No. 61261130588), Tsinghua University Initiative Scientific Research Program (No. 20131089256) and THU-NUS NExT Co-Lab.

\bibliographystyle{splncs03}
\bibliography{paper}

%\end{CJK*}
\end{document}

